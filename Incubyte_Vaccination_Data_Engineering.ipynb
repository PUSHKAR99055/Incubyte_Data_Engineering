{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Vaccine Analysis\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_frame_Creation(df):\n",
    "    if df!=None:\n",
    "        assert df.count() == 3\n",
    "        \n",
    "def test_Data_frame_data(df):\n",
    "    if df!=None:\n",
    "        assert len(df.columns) == 4\n",
    "    if df.count() > df.dropDuplicates([\"Name\", \"VaccinationType\", \"VaccinationDate\", \"Country\"]).count():\n",
    "        raise ValueError('Data has duplicates')\n",
    "        \n",
    "def test_Data_frame_Agg_logic(df1,df2,df3,df4):\n",
    "    if df1!=None and df2!=None and df3!=None and df4!=None:\n",
    "        assert df1.count()+df2.count()+df3.count() == df4.count()\n",
    "        \n",
    "def test_Data_frame_vacc_count(df,df1):\n",
    "    if df!=None and df1!=None:\n",
    "        assert df1.select(sum(\"count\")).collect()[0][0] == df.count()\n",
    "        \n",
    "def test_Data_frame_percent_vaccinated_logic(df):\n",
    "    if df!=None:\n",
    "        assert df.collect()[0][1] == 30\n",
    "        \n",
    "def test_Data_frame_percent_contri_logic(df):\n",
    "    if df!=None:\n",
    "        assert int(df.collect()[0][1]) == 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV File\n",
    "IND_DF = spark.read.csv(\"hdfs://nameservice1/user/adityabend99edu/IND.csv\", header='true', \n",
    "                      inferSchema='true')\n",
    "USA_DF = spark.read.csv(\"hdfs://nameservice1/user/adityabend99edu/USA.csv\", header='true', \n",
    "                      inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUS_DF = spark.read.csv(\"hdfs://nameservice1/user/adityabend99edu/AUS.csv\", header='true', \n",
    "                      inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting xlsx file to csv and then reading\n",
    "df3 = pandas.read_excel('AUS.xlsx') \n",
    "df3.to_csv('AUS.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------------+-------+\n",
      "|     Name|VaccinationType|    VaccinationDate|Country|\n",
      "+---------+---------------+-------------------+-------+\n",
      "|     Mike|            LMN|2022-05-11 00:00:00|    AUS|\n",
      "|Jonnathan|            XYZ|         2021-13-13|    AUS|\n",
      "| Cristina|            ABC|2022-03-12 00:00:00|    AUS|\n",
      "+---------+---------------+-------------------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Data Cleansing\n",
    "AUS_DF = AUS_DF.withColumn(\"Country\", lit(\"AUS\")).withColumnRenamed(\"Vaccine Type\", \"VaccinationType\").withColumnRenamed(\"Patient Name\", \"Name\").withColumnRenamed(\"Date of Vaccination\", \"VaccinationDate\").drop(\"Date of Birth\",\"Unique ID\")\n",
    "\n",
    "AUS_DF.show()\n",
    "\n",
    "#Testing\n",
    "print(test_Data_frame_data(AUS_DF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------+-------+\n",
      "|Name|VaccinationType|VaccinationDate|Country|\n",
      "+----+---------------+---------------+-------+\n",
      "| Sam|            EFG|        6152022|    USA|\n",
      "|John|            XYZ|        1052022|    USA|\n",
      "|Mike|            ABC|       12282021|    USA|\n",
      "+----+---------------+---------------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Data Cleansing\n",
    "USA_DF = USA_DF.withColumn(\"Country\", lit(\"USA\")).drop(\"ID\")\n",
    "\n",
    "USA_DF.show()\n",
    "\n",
    "#Testing\n",
    "print(test_Data_frame_data(USA_DF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-------------------+-------+\n",
      "|  Name|VaccinationType|    VaccinationDate|Country|\n",
      "+------+---------------+-------------------+-------+\n",
      "| Vikas|            XYZ|2022-01-01 00:00:00|    IND|\n",
      "| Rahul|            ABC|2022-03-05 00:00:00|    IND|\n",
      "|Sameer|            ABC|2022-02-20 00:00:00|    IND|\n",
      "+------+---------------+-------------------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Data Cleansing\n",
    "IND_DF = IND_DF.drop(\"DOB\",\"Free or Paid\",\"ID\")\n",
    "IND_DF = IND_DF.withColumn(\"Country\", lit(\"IND\"))\n",
    "\n",
    "IND_DF.show()\n",
    "\n",
    "#Testing\n",
    "print(test_Data_frame_data(IND_DF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------------+-------+\n",
      "|     Name|VaccinationType|    VaccinationDate|Country|\n",
      "+---------+---------------+-------------------+-------+\n",
      "|     Mike|            LMN|2022-05-11 00:00:00|    AUS|\n",
      "|Jonnathan|            XYZ|         2021-13-13|    AUS|\n",
      "| Cristina|            ABC|2022-03-12 00:00:00|    AUS|\n",
      "|    Vikas|            XYZ|2022-01-01 00:00:00|    IND|\n",
      "|    Rahul|            ABC|2022-03-05 00:00:00|    IND|\n",
      "|   Sameer|            ABC|2022-02-20 00:00:00|    IND|\n",
      "|      Sam|            EFG|            6152022|    USA|\n",
      "|     John|            XYZ|            1052022|    USA|\n",
      "|     Mike|            ABC|           12282021|    USA|\n",
      "+---------+---------------+-------------------+-------+\n",
      "\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Data merging into single source of truth\n",
    "dfs = [AUS_DF,IND_DF,USA_DF]\n",
    "Combine_df = reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "Combine_df.show()\n",
    "\n",
    "#Testing\n",
    "print(test_Data_frame_data(Combine_df))\n",
    "print(test_Data_frame_Agg_logic(AUS_DF,IND_DF,USA_DF,Combine_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+-----+\n",
      "|Country|VaccinationType|count|\n",
      "+-------+---------------+-----+\n",
      "|    AUS|            ABC|    1|\n",
      "|    AUS|            XYZ|    1|\n",
      "|    IND|            ABC|    2|\n",
      "|    USA|            XYZ|    1|\n",
      "|    AUS|            LMN|    1|\n",
      "|    IND|            XYZ|    1|\n",
      "|    USA|            EFG|    1|\n",
      "|    USA|            ABC|    1|\n",
      "+-------+---------------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Total vaccination count by country and vaccination type\n",
    "Vaccination_Count_By_Country = df.select(\"Country\",\"VaccinationType\").groupBy(\"Country\", \"VaccinationType\").count()\n",
    "\n",
    "Vaccination_Count_By_Country.show()\n",
    "\n",
    "#Testing\n",
    "print(test_Data_frame_vacc_count(Combine_df,Vaccination_Count_By_Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|Country|perecent_Vaccinated|\n",
      "+-------+-------------------+\n",
      "|    AUS|               30.0|\n",
      "|    USA|               30.0|\n",
      "|    IND|               30.0|\n",
      "+-------+-------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# %vaccination in each country (You can assume values for total population) \n",
    "# Let Total Population Count be 10\n",
    "total_pop = 10\n",
    "Vaccination_Count_By_Country.createOrReplaceTempView(\"QUERY_TABLE\")\n",
    "Percentage_Vaccinated_By_Country = spark.sql(\"select Country, sum(count) * 100 /{0} as perecent_Vaccinated  from QUERY_TABLE  group by Country\".format(total_pop))\n",
    "\n",
    "Percentage_Vaccinated_By_Country.show()\n",
    "\n",
    "#Testing\n",
    "print(test_Data_frame_percent_vaccinated_logic(Percentage_Vaccinated_By_Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|Country|    percent_contri|\n",
      "+-------+------------------+\n",
      "|    AUS|33.333333333333336|\n",
      "|    USA|33.333333333333336|\n",
      "|    IND|33.333333333333336|\n",
      "+-------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# vaccination contribution by country (Sum of percentages add up to 100)\n",
    "df1 = Vaccination_Count_By_Country.select(\"Country\",\"count\").groupBy(\"Country\").sum()\n",
    "df1 = df1.withColumnRenamed(\"sum(count)\",\"count\")\n",
    "df1.createOrReplaceTempView(\"QUERY_TABLE\")\n",
    "Percentage_Contribution_By_Country = spark.sql(\"select Country, count * 100/(select sum(count) from QUERY_TABLE ) as percent_contri  from QUERY_TABLE\")\n",
    "\n",
    "Percentage_Contribution_By_Country.show()\n",
    "\n",
    "#Testing\n",
    "print(test_Data_frame_percent_contri_logic(Percentage_Contribution_By_Country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
